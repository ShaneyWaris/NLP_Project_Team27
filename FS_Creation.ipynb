{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\meet1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\meet1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import re, nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow_hub as hub\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "base = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shaney Features\n",
    "\n",
    "def mark_negation(sentence):\n",
    "        negation = r\"\"\"(?:^(?:never|no|nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt\n",
    "        |dont|doesnt|didnt|isnt|arent|aint)$)|n't \"\"\"\n",
    "        neg_re = re.compile(negation, re.VERBOSE)\n",
    "        punctuation = r\"^[.:;!?]$\"\n",
    "        puncts = re.compile(punctuation)\n",
    "\n",
    "        doc = word_tokenize(sentence)\n",
    "        neg_scope = False\n",
    "        count = 0\n",
    "        for i, word in enumerate(doc):\n",
    "            if neg_re.search(word):\n",
    "                if not neg_scope:\n",
    "                    neg_scope = not neg_scope\n",
    "                    continue\n",
    "                else:\n",
    "                    doc[i] += \"_NEG\"\n",
    "                    count += 1\n",
    "            elif neg_scope and puncts.search(word):\n",
    "                neg_scope = not neg_scope\n",
    "            elif neg_scope and not puncts.search(word):\n",
    "                doc[i] += \"_NEG\"\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "# Bing lui lexicons - no. of positive & negative words in a tweet.\n",
    "def bing_lui(tweet):\n",
    "    positive_words = 0\n",
    "    negative_words = 0\n",
    "    positive_minus_negative = 0\n",
    "    with open(base + 'Lex/bing lui lexicon/positive-words.txt') as file:\n",
    "        contents = file.read()\n",
    "        for word in tweet.split():\n",
    "            if word in contents:\n",
    "                positive_words += 1\n",
    "    with open(base + 'Lex/bing lui lexicon/negative-words.txt') as file:\n",
    "        contents = file.read()\n",
    "        for word in tweet.split():\n",
    "            if word in contents:\n",
    "                negative_words += 1\n",
    "\n",
    "    positive_minus_negative = positive_words - negative_words\n",
    "    lexicon_vector = [positive_words, negative_words, positive_minus_negative]\n",
    "    return lexicon_vector\n",
    "\n",
    "def polarity(sentence) :\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    pol = sid.polarity_scores(sentence)\n",
    "    return [pol['pos'], pol['neg'], pol['neu']]\n",
    "\n",
    "def get_shaney_features(sentence):\n",
    "    negation_words = mark_negation(sentence)\n",
    "    bingLui = bing_lui(sentence)\n",
    "    polari = polarity(sentence)\n",
    "    return np.array([negation_words] + bingLui + polari).reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sriza Features\n",
    "\n",
    "\"\"\"##No of URLS\"\"\"\n",
    "\n",
    "def no_of_URLs(listoftweets):\n",
    "    count=[]\n",
    "    for tweet in listoftweets:\n",
    "        l=re.findall(r'((www\\.[\\S]+)|(https?:\\/\\/[\\S]+))',tweet)\n",
    "        #print(l)\n",
    "        count.append(len(l))\n",
    "    return np.array(count);\n",
    "\n",
    "\n",
    "\"\"\"##No of Hashtags\"\"\"\n",
    "\n",
    "def no_of_hashtags(listoftweets):\n",
    "    count=[]\n",
    "    for tweet in listoftweets:\n",
    "        l=re.findall(r'#(\\w+)',tweet)\n",
    "        #print(l)\n",
    "        count.append(len(l))\n",
    "    return np.array(count);\n",
    "\n",
    "\n",
    "\"\"\"##Lexicon sentiment of hastags\"\"\"\n",
    "\n",
    "def aggregatepolarityscores_hashtags(listoftweets):\n",
    "    with open(base + 'Lex/unigrams-pmilexiconNRC_HashtagsSentiment.txt', 'r') as document:\n",
    "        hashtagscore = {}\n",
    "        for line in document:\n",
    "            line = line.split()\n",
    "            if(line[0][0]=='@'):\n",
    "                continue\n",
    "            hashtagscore[line[0]] = float(line[1:][0])\n",
    "\n",
    "    vector=[]\n",
    "    for tweet in listoftweets:\n",
    "        tweet=tweet.split(' ')\n",
    "        #print(tweet)\n",
    "        \n",
    "        val1=0\n",
    "        val=0\n",
    "        for word in tweet:\n",
    "            if len(word)>1 and word[0]!='#':\n",
    "                continue\n",
    "            if word in hashtagscore.keys():\n",
    "                val = hashtagscore[word]\n",
    "            val1+=val\n",
    "        \n",
    "        vector.append(val1)\n",
    "    return np.array(vector)\n",
    "\n",
    "\n",
    "\"\"\"##No of Emojis\"\"\"\n",
    "\n",
    "def no_of_emojis(listoftweets):\n",
    "    with open(base + 'Lex/AffinnEmoticons.txt', 'r',encoding='UTF-8') as document:\n",
    "        emoticons_score1={}\n",
    "        #print(document)\n",
    "        for line in document:\n",
    "            #print(line)\n",
    "            words = line.split()\n",
    "            emoticons_score1[words[0]]=int(words[1])\n",
    "    emocount=[]\n",
    "    for tweet in listoftweets:\n",
    "        emo=0\n",
    "        t=tweet.split(' ')\n",
    "        \n",
    "        for word in t:\n",
    "            if word in emoticons_score1.keys():\n",
    "                emo+=1\n",
    "                #print(word)\n",
    "        emocount.append(emo)\n",
    "    return np.array(emocount)\n",
    "\n",
    "\"\"\"##Emoji Sentiment Average\"\"\"\n",
    "\n",
    "def emoticons_score(listoftweets):\n",
    "    with open(base + 'Lex/AffinnEmoticons.txt', 'r',encoding='UTF-8') as document:\n",
    "        emoticons_score1={}\n",
    "        #print(document)\n",
    "        for line in document:\n",
    "            #print(line)\n",
    "            words = line.split()\n",
    "            emoticons_score1[words[0]]=int(words[1])\n",
    "    vector=[]\n",
    "    for tweet in listoftweets:\n",
    "        tweet=tweet.split(' ')\n",
    "        emoscore=0\n",
    "        for word in tweet:\n",
    "            if word in emoticons_score1.keys():\n",
    "                emoscore+=emoticons_score1[word]\n",
    "        #print(emoscore)\n",
    "        #feature_vector[i][17]+=emoscore \n",
    "        vector.append(emoscore)\n",
    "    return np.array(vector)\n",
    "\n",
    "def getfeaturearray(listoftweets):\n",
    "\n",
    "    v1=no_of_URLs(listoftweets)\n",
    "    v2=no_of_hashtags(listoftweets)\n",
    "    v3=aggregatepolarityscores_hashtags(listoftweets)\n",
    "    v4=no_of_emojis(listoftweets)\n",
    "    v5=emoticons_score(listoftweets)\n",
    "\n",
    "    return np.concatenate((v1.reshape(-1,1),v2.reshape(-1,1),v3.reshape(-1,1),v4.reshape(-1,1),v5.reshape(-1,1)),axis=1).reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dict = {'agree' : 0, 'disagree' : 1, 'discuss' : 2, 'unrelated' :3}\n",
    "\n",
    "def create_embeddings(emdeb, train_body_filename, train_stance_filename) :\n",
    "    df1 = pd.read_csv(train_body_filename)\n",
    "    df2 = pd.read_csv(train_stance_filename)\n",
    "    X = np.zeros((df2.shape[0], 1*(512+12)))\n",
    "    y = np.zeros(df2.shape[0])\n",
    "    i = 0\n",
    "    for index in df1.index :\n",
    "        a = df1.loc[df1['Body ID'] == index][['articleBody']].values\n",
    "        if a.shape == (0,1) :\n",
    "            continue\n",
    "        print(index, end=' ')\n",
    "        for k in df2.loc[df2['Body ID'] == index].values :\n",
    "#             print(k[0] + a[0][0])\n",
    "            full = k[0] + a[0][0]\n",
    "            e = emdeb([full]).numpy().reshape((1,-1))\n",
    "            sh = get_shaney_features(full)\n",
    "            sr = getfeaturearray([full])\n",
    "            X[i] = np.concatenate((e, sr, sh), axis=1)\n",
    "            y[i] = y_dict[k[-1]]\n",
    "            i += 1\n",
    "    return X[:i], y[:i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using C:\\Users\\meet1\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SentenceBERT LaBSE\n",
    "\n",
    "train_X, train_y = create_embeddings(sbert_model.encode, '../train_bodies.txt', '../train_stances.txt')\n",
    "np.savetxt('train_instance_bert.csv', np.concatenate((train_X, train_y.reshape((-1,1))), axis=1), delimiter=',')\n",
    "test_X, test_y = create_embeddings(sbert_model.encode, '../competition_test_bodies.txt', '../competition_test_stances.txt')\n",
    "np.savetxt('test_instance_bert.csv', np.concatenate((test_X, test_y.reshape((-1,1))), axis=1), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Universal Encoder\n",
    "\n",
    "train_X, train_y = create_embeddings(embed, '../train_bodies.txt', '../train_stances.txt')\n",
    "np.savetxt('train_instance_u.csv', np.concatenate((train_X, train_y.reshape((-1,1))), axis=1), delimiter=',')\n",
    "test_X, test_y = create_embeddings(embed, '../competition_test_bodies.txt', '../competition_test_stances.txt')\n",
    "np.savetxt('test_instance_u.csv', np.concatenate((test_X, test_y.reshape((-1,1))), axis=1), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
